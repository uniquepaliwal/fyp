const { BufferMemory } = require("langchain/memory");
const { ChatOpenAI } = require("@langchain/openai");
const { ConversationChain } = require("langchain/chains");



async function response_from_gpt(userQuery, userMemory) {
    const openapikey = process.env.OPENAI_API_KEY;
    const chatModel = new ChatOpenAI({
        openAIApiKey: openapikey,
        modelName: "gpt-4",
    });
    const memory = new BufferMemory();
    const systemPrompt = `
    identity:
        description: "You are IconBot, a chatbot created by Cozmotec for Icon Accounting, designed as a support chatbot."
    response_type:
        - The response must be in JSON format: {"gpt_solution":""}
        - do not append GPT: , in your answer
    guidelines:
        - If asked about your identity, respond professionally as IconBot and never mention OpenAI in your response.
        - The question or query will always come from an end-user (role id 2).
        - In the conversation if the user is telling his first name, last name or email, then the response type will change slightly.
           -- "gpt_solution" will say a message telling user we got his details ( right it awsomely ). 
           -- add a "type" in the response with value 1,
           -- add "data" key which will have a object which will contain the keys first_name , last_name and email_id, if any of these values are not given then keep the key empty.
    information:
        - Role id 1 is for Support Staff.
        - Role id 2 is for the End User.
        - Role id 3 is for old GPT responses.
    `;
    const conversation = new ConversationChain({
        llm: chatModel,
        memory: memory,
    });
    await memory.chatHistory.addUserMessage(systemPrompt);

    // Add the old conversation history (userMemory) to the memory
    if (userMemory && userMemory.length > 0) {
        for (let i = 0; i < userMemory.length - 1; i++) {
            // for (const message of userMemory) {
            if (userMemory[i].role_id == 1) {
                await memory.chatHistory.addUserMessage(`Support staff reply: ${userMemory[i].content}`);
            } else if (userMemory[i].role_id == 2) {
                await memory.chatHistory.addUserMessage(`${userMemory[i].content}`);
            } else if (userMemory[i].role_id == 3) {
                await memory.chatHistory.addAIChatMessage(`{"gpt_solution":"${userMemory[i].content}"}`);
            }
        }
    }

    // Add the latest user query to the memory (assuming it's from an End User)
    await memory.chatHistory.addUserMessage(`End User: ${userQuery}`);

    try {
        console.log("Sending request to GPT with context:", memory.chatHistory.messages);

        // Generate a response using the conversation chain
        const response = await conversation.call({ input: userQuery });

        // Ensure the response is in JSON format
        return (response.response);
    } catch (err) {
        console.error("Error while retrieving and generating response from OpenAI:", err);
        throw new Error(`Error while retrieving and generating response from OpenAI: ${err.message}`);
    }
}

exports.final_response = async function (options) {
    try {
        options = this.parse(options)
        const userQuery = JSON.stringify(options.userQuery);
        const userMemory = options.userMemory
        if (!userQuery) {
            throw new Error('User Query is not present');
        }
        const response = await response_from_gpt(userQuery, userMemory)
        console.log(response)
        return JSON.parse(response);
    } catch (error) {
        console.log(`Error in final response - ${error.message}`)
    }
}

// old message
// const OpenAI = require('openai');
// const openai = new OpenAI();
/**
 * Generates a response from GPT based on the query and database response.
 * @param {string} responseFromQuery - The result of the database query.
 * @param {string} originalQuestionAsked - The original question asked by the user.
 * @returns {string} - The response generated by GPT.
 */
// async function response_from_gpt(userQuery, userMemory) {
//     console.log(typeof userMemory)
//     console.log(userMemory)
//     const question = `
//     context:
//         question: "${userQuery}"
//     identity:
//         description: "You are IconBot, a chatbot created by Cozmotec for Icon Accouting for beging a support chatbot."
//     response_type:
//         - response type will be json of format - {"gpt_solution":"" }
//     guidelines:
//         - If asked about your identity, respond professionally as IconBot, neve mention OpenAI in your response.`;


//     try {
//         console.log(question)
//         const completion = await openai.chat.completions.create({
//             messages: [{ role: "system", content: question }],
//             model: "gpt-4o-mini",
//         });
//         if (completion && completion.choices && completion.choices.length > 0) return completion.choices[0].message.content
//         else console.log('No completion choices returned');
//     } catch (err) {
//         throw new Error(`Error while retrieving and generating response from openAi ${err}`)
//     }
// }
